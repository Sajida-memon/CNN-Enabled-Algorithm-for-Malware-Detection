
Synthetic Data Generation:

The generate_synthetic_data function creates a dataset of binary sequences (representing malware features) with corresponding labels (0 for phishing, 1 for trojan horse, and 2 for denial of service).
Model Architecture:

The model consists of two convolutional layers followed by max pooling layers to reduce the feature space. After flattening, two dense layers are added with a final output layer that uses softmax activation for classification.
Training:

The model is trained for 10 epochs with a batch size of 16. Validation is done using a separate test set.
Evaluation:

After training, the model's accuracy on the test set is printed.
This is a basic example. In a real-world scenario, the dataset would need to be much larger and include actual features derived from malware samples. Additionally, more advanced data preprocessing, feature extraction, and model tuning would be required for production-grade malware detection.















import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# Generate a synthetic dataset
def generate_synthetic_data(num_samples=5000, sequence_length=100):
    np.random.seed(42)
    # Random binary sequences for malware features
    X = np.random.randint(0, 2, (num_samples, sequence_length))
    # Labels: 0 = phishing, 1 = trojan horse, 2 = denial of service
    y = np.random.choice([0, 1, 2], num_samples)
    return X, y

# Load data
X, y = generate_synthetic_data()

# Preprocess data
X = X.reshape(-1, X.shape[1], 1)  # Reshape for Conv1D input
y = to_categorical(y)  # Convert labels to categorical format

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define CNN model
model = Sequential()

# Add Convolutional layers
model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(32, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))

# Flatten and add Dense layers
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(3, activation='softmax'))  # 3 classes for phishing, trojan horse, denial of service

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model and store the training history
history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy:.2f}')

# Plot training & validation accuracy values
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Test'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Test'], loc='upper left')

plt.tight_layout()
plt.show()
